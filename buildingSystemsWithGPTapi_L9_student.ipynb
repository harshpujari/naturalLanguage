{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YkzdtSTui861"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "import sys\n",
        "sys.path.append('../..')\n",
        "import utils\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "_ = load_dotenv(find_dotenv()) # read local .env file\n",
        "\n",
        "openai.api_key  = os.environ['OPENAI_API_KEY']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_completion_from_messages(messages, model=\"gpt-3.5-turbo\", temperature=0, max_tokens=500):\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=temperature,\n",
        "        max_tokens=max_tokens,\n",
        "    )\n",
        "    return response.choices[0].message[\"content\"]"
      ],
      "metadata": {
        "id": "njPBVggojFbi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=0, # this is the degree of randomness of the model's output\n",
        "    )\n",
        "    return response.choices[0].message[\"content\"]"
      ],
      "metadata": {
        "id": "Hx_vBUIMjHo3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = f\"\"\"\n",
        "You should express what you want a model to do by \\\n",
        "providing instructions that are as clear and \\\n",
        "specific as you can possibly make them. \\\n",
        "This will guide the model towards the desired output, \\\n",
        "and reduce the chances of receiving irrelevant \\\n",
        "or incorrect responses. Don't confuse writing a \\\n",
        "clear prompt with writing a short prompt. \\\n",
        "In many cases, longer prompts provide more clarity \\\n",
        "and context for the model, which can lead to \\\n",
        "more detailed and relevant outputs.\n",
        "\"\"\"\n",
        "prompt = f\"\"\"\n",
        "Summarize the text delimited by triple backticks \\\n",
        "into a single sentence.\n",
        "```{text}```\n",
        "\"\"\"\n",
        "response = get_completion(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "GCjo9k_hjLAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"\"\"\n",
        "Generate a list of three made-up book titles along \\\n",
        "with their authors and genres.\n",
        "Provide them in JSON format with the following keys:\n",
        "book_id, title, author, genre.\n",
        "\"\"\"\n",
        "response = get_completion(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "SQZ1ykN9jN9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "customer_msg = f\"\"\"\n",
        "tell me about the smartx pro phone and the fotosnap camera, the dslr one.\n",
        "Also, what TVs or TV related products do you have?\"\"\"\n",
        "\n",
        "products_by_category = utils.get_products_from_query(customer_msg)\n",
        "category_and_product_list = utils.read_string_to_list(products_by_category)\n",
        "product_info = utils.get_mentioned_product_info(category_and_product_list)\n",
        "assistant_answer = utils.answer_user_msg(user_msg=customer_msg,\n",
        "                                                   product_info=product_info)"
      ],
      "metadata": {
        "id": "e7lOEFtOjQMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(assistant_answer)"
      ],
      "metadata": {
        "id": "qw_tORHqjTKd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cust_prod_info = {\n",
        "    'customer_msg': customer_msg,\n",
        "    'context': product_info\n",
        "}"
      ],
      "metadata": {
        "id": "gsWCCd9HjU21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_with_rubric(test_set, assistant_answer):\n",
        "\n",
        "    cust_msg = test_set['customer_msg']\n",
        "    context = test_set['context']\n",
        "    completion = assistant_answer\n",
        "\n",
        "    system_message = \"\"\"\\\n",
        "    You are an assistant that evaluates how well the customer service agent \\\n",
        "    answers a user question by looking at the context that the customer service \\\n",
        "    agent is using to generate its response.\n",
        "    \"\"\"\n",
        "\n",
        "    user_message = f\"\"\"\\\n",
        "You are evaluating a submitted answer to a question based on the context \\\n",
        "that the agent uses to answer the question.\n",
        "Here is the data:\n",
        "    [BEGIN DATA]\n",
        "    ************\n",
        "    [Question]: {cust_msg}\n",
        "    ************\n",
        "    [Context]: {context}\n",
        "    ************\n",
        "    [Submission]: {completion}\n",
        "    ************\n",
        "    [END DATA]\n",
        "\n",
        "Compare the factual content of the submitted answer with the context. \\\n",
        "Ignore any differences in style, grammar, or punctuation.\n",
        "Answer the following questions:\n",
        "    - Is the Assistant response based only on the context provided? (Y or N)\n",
        "    - Does the answer include information that is not provided in the context? (Y or N)\n",
        "    - Is there any disagreement between the response and the context? (Y or N)\n",
        "    - Count how many questions the user asked. (output a number)\n",
        "    - For each question that the user asked, is there a corresponding answer to it?\n",
        "      Question 1: (Y or N)\n",
        "      Question 2: (Y or N)\n",
        "      ...\n",
        "      Question N: (Y or N)\n",
        "    - Of the number of questions asked, how many of these questions were addressed by the answer? (output a number)\n",
        "\"\"\"\n",
        "\n",
        "    messages = [\n",
        "        {'role': 'system', 'content': system_message},\n",
        "        {'role': 'user', 'content': user_message}\n",
        "    ]\n",
        "\n",
        "    response = get_completion_from_messages(messages)\n",
        "    return response"
      ],
      "metadata": {
        "id": "Pt4Mz0BKjXZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_output = eval_with_rubric(cust_prod_info, assistant_answer)\n",
        "print(evaluation_output)"
      ],
      "metadata": {
        "id": "oTiMFKKjjXWe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}